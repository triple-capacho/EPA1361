{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model vulnerability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scenario discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import prim\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from ema_workbench import ema_logging, MultiprocessingEvaluator\n",
    "from ema_workbench import Model, RealParameter, ScalarOutcome, CategoricalParameter, IntegerParameter, BooleanParameter\n",
    "from ema_workbench.em_framework.samplers import sample_uncertainties\n",
    "# from ema_workbench.em_framework.evaluators import MC\n",
    "from dike_model_function import DikeNetwork\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "# dike_model, planning_steps = get_model_for_problem_formulation(5)\n",
    "\n",
    "from ema_workbench.util.utilities import load_results\n",
    "\n",
    "results = load_results('./martijnmc10pol1000scen.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do something with the results to make them applicable (maybe)\n",
    "experiments, outcomes = results\n",
    "\n",
    "x = experiments\n",
    "y = outcomes # or all, or some other specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_alg = prim.Prim(x, y, threshold=0.8, peel_alpha=0.1)\n",
    "box1 = prim_alg.find_box()\n",
    "\n",
    "box1.show_tradeoff()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.inspect_tradeoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select any to inspect (currently 21)\n",
    "box1.inspect(21)\n",
    "box1.inspect(21, style='graph')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.select(21)\n",
    "fig = box1.show_pairs_scatter()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some variable and reinspect\n",
    "box1.drop_restriction('Cellulosic cost')\n",
    "box1.inspect(style='graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subspace partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import feature_scoring\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fs = feature_scoring.get_feature_scores_all(experiments, outcomes)\n",
    "sns.heatmap(fs, cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import dimensional_stacking\n",
    "dimensional_stacking.create_pivot_plot(x,y, 2, nbins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sensitivity analysis \n",
    "\n",
    "Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "\n",
    "with SequentialEvaluator(py_model) as evaluator:\n",
    "    sa_results = evaluator.perform_experiments(scenarios = 50 ,\n",
    "                                               uncertainty_sampling='sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sa_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0715329461a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutcomes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sa_results' is not defined"
     ]
    }
   ],
   "source": [
    "experiments, outcomes = sa_results\n",
    "outcomes = {key:outcomes[key][:,0,:] for key in outcomes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = get_SALib_problem(py_model.uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si = sobol.analyze(problem, outcomes['prey'][:,-1],\n",
    "                   calc_second_order=True, print_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_filter = {k:Si[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Si_df = pd.DataFrame(Si_filter, index = problem['names'])\n",
    "Si_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_results = sa_results_1\n",
    "\n",
    "tree_experiments, tree_outcomes = extra_trees_results\n",
    "tree_outcomes = {key:tree_outcomes[key][:,0, -1] for key in tree_outcomes.keys()} # Important for printing, delete redundant array\n",
    "tree_experiments = tree_experiments.drop(columns=['scenario', 'policy', 'model'], inplace=False)\n",
    "print(tree_experiments.head(5))\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench import ema_logging\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "score = feature_scoring.get_feature_scores_all(tree_experiments, tree_outcomes)\n",
    "sns.heatmap(score, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sobol vs. extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, p_values = feature_scoring.get_rf_feature_scores(tree_experiments,tree_outcomes['prey'],  mode=RuleInductionType.REGRESSION, \n",
    "                                                         nr_trees=100, max_features=0.6)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Si_df_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-32c40d3c2659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSi_df_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Si_df_1' is not defined"
     ]
    }
   ],
   "source": [
    "Si_df_1['ST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
